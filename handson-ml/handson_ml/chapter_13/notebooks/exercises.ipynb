{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4aa2e9-8b82-4f54-8555-c46365db88ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T14:18:00.128855Z",
     "iopub.status.busy": "2024-02-17T14:18:00.128245Z",
     "iopub.status.idle": "2024-02-17T14:18:02.553939Z",
     "shell.execute_reply": "2024-02-17T14:18:02.553197Z",
     "shell.execute_reply.started": "2024-02-17T14:18:00.128787Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 20:03:00.347887: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-17 20:03:00.629432: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/twogoodap/Coding_Playground/Machine_Learning/Hands_on_Machine_Learning/handson-ml/mlruns\n",
      "/home/twogoodap/Coding_Playground/Machine_Learning/Hands_on_Machine_Learning/handson-ml/handson_ml/chapter_13/dataset\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data, keras\n",
    "\n",
    "ROOT_DIR = Path().absolute().parent\n",
    "MLRUNS_DIR = ROOT_DIR.parents[1] / \"mlruns\"\n",
    "DATA_DIR = ROOT_DIR / \"dataset\"\n",
    "PROTO_DIR = ROOT_DIR / \"protobufs\"\n",
    "TFR_DIR = DATA_DIR / \"tfrecords\"\n",
    "IMDB_DIR = DATA_DIR / \"large_movie_review\"\n",
    "\n",
    "if not TFR_DIR.is_dir():\n",
    "    TFR_DIR.mkdir(parents=True)\n",
    "if not PROTO_DIR.is_dir():\n",
    "    PROTO_DIR.mkdir(parents=True)\n",
    "\n",
    "print(f\"{MLRUNS_DIR}\\n{DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96619b4-58cf-48c3-8f89-9360039455c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T14:18:02.556695Z",
     "iopub.status.busy": "2024-02-17T14:18:02.556576Z",
     "iopub.status.idle": "2024-02-17T14:18:03.158234Z",
     "shell.execute_reply": "2024-02-17T14:18:03.157585Z",
     "shell.execute_reply.started": "2024-02-17T14:18:02.556683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/twogoodap/Coding_Playground/Machine_Learning/Hands_on_Machine_Learning/handson-ml/handson_ml/chapter_13/mlruns/2', creation_time=1699089661167, experiment_id='2', last_update_time=1699089661167, lifecycle_stage='active', name='tf_data_api', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(f\"sqlite:///{MLRUNS_DIR}/mlflow.db\")\n",
    "mlflow.set_experiment(\"tf_data_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c368e5-b864-4a1c-b119-cec8d8de262e",
   "metadata": {},
   "source": [
    "## 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef663e6f-b917-4741-91da-d57074429f25",
   "metadata": {},
   "source": [
    "```proto\n",
    "syntax = \"proto3\";\n",
    "\n",
    "message BytesList { repeated bytes value = 1; }\n",
    "message FloatList { repeated float value = 1 [packed = true]; }\n",
    "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
    "message Feature {\n",
    "    oneof kind {\n",
    "        BytesList bytes_list = 1;\n",
    "        FloatList float_list = 2;\n",
    "        Int64List int64_list = 3;\n",
    "    }\n",
    "};\n",
    "message Features { map<string, Feature> feature = 1; };\n",
    "message Example { Features features = 1; };\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "80e57c12-d1e2-4934-99c2-86f3291daf23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T11:38:07.772560Z",
     "iopub.status.busy": "2024-02-17T11:38:07.771880Z",
     "iopub.status.idle": "2024-02-17T11:38:07.792784Z",
     "shell.execute_reply": "2024-02-17T11:38:07.790849Z",
     "shell.execute_reply.started": "2024-02-17T11:38:07.772497Z"
    }
   },
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.train import BytesList, Example, Feature, Features, Int64List\n",
    "\n",
    "\n",
    "def fashion_mnist_to_tfrecord(\n",
    "    dataset: data.Dataset,\n",
    "    name: str,\n",
    "    record_dir: Path = TFR_DIR / \"fashion_mnist\",\n",
    "    n_shards: int = 10,\n",
    "    seed: int = 42,\n",
    ") -> list[str]:\n",
    "    def __fashion_mnist_example(image: np.ndarray | tf.Tensor, label: str) -> Example:\n",
    "        return Example(\n",
    "            features=Features(\n",
    "                feature={\n",
    "                    \"image\": Feature(\n",
    "                        bytes_list=BytesList(\n",
    "                            value=[tf.io.serialize_tensor(image).numpy()]\n",
    "                        )\n",
    "                    ),\n",
    "                    \"label\": Feature(int64_list=Int64List(value=[int(label)])),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    (record_dir / name).mkdir(parents=True, exist_ok=True)\n",
    "    paths = [\n",
    "        str(record_dir / name / f\"{shard:03}.tfrecord\") for shard in range(n_shards)\n",
    "    ]\n",
    "\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path)) for path in paths]\n",
    "\n",
    "        for i, (img, lbl) in dataset.enumerate():\n",
    "            writers[i % n_shards].write(\n",
    "                __fashion_mnist_example(img, lbl).SerializeToString()\n",
    "            )\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a1f18ea5-dfe8-42b6-8b13-9eece0510673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T11:38:10.754942Z",
     "iopub.status.busy": "2024-02-17T11:38:10.754776Z",
     "iopub.status.idle": "2024-02-17T11:38:15.699001Z",
     "shell.execute_reply": "2024-02-17T11:38:15.698310Z",
     "shell.execute_reply.started": "2024-02-17T11:38:10.754928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96135"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BUFFER_SIZE = 10_000\n",
    "SEED = 42\n",
    "\n",
    "images, targets = (\n",
    "    (fmnist := fetch_openml(name=\"Fashion-MNIST\", as_frame=False, parser=\"auto\")).data,\n",
    "    fmnist.target,\n",
    ")\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    images.reshape(-1, 28, 28).astype(\"uint8\"),\n",
    "    targets,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=targets,\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=SEED, stratify=y_train_full\n",
    ")\n",
    "del X_train_full, y_train_full\n",
    "\n",
    "train_set, val_set, test_set = (\n",
    "    data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(\n",
    "        buffer_size=BUFFER_SIZE\n",
    "    ),\n",
    "    data.Dataset.from_tensor_slices((X_val, y_val)),\n",
    "    data.Dataset.from_tensor_slices((X_test, y_test)),\n",
    ")\n",
    "\n",
    "del X_train, y_train, X_val, y_val, X_test, y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "9da42e90-8cca-446d-8619-57a701fe363a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T11:38:15.699965Z",
     "iopub.status.busy": "2024-02-17T11:38:15.699840Z",
     "iopub.status.idle": "2024-02-17T11:38:32.143418Z",
     "shell.execute_reply": "2024-02-17T11:38:32.142841Z",
     "shell.execute_reply.started": "2024-02-17T11:38:15.699953Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 17:23:15.705397: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-02-17 17:23:25.743780: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-02-17 17:23:28.697177: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4133"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths = fashion_mnist_to_tfrecord(train_set, name=\"train\")\n",
    "val_paths = fashion_mnist_to_tfrecord(val_set, name=\"validation\")\n",
    "test_paths = fashion_mnist_to_tfrecord(test_set, name=\"test\")\n",
    "\n",
    "del train_set, val_set, test_set\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "de5bdce8-5163-4bde-a879-884bb91e1080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T11:38:32.148950Z",
     "iopub.status.busy": "2024-02-17T11:38:32.148832Z",
     "iopub.status.idle": "2024-02-17T11:38:32.616706Z",
     "shell.execute_reply": "2024-02-17T11:38:32.616369Z",
     "shell.execute_reply.started": "2024-02-17T11:38:32.148937Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 17:23:32.581657: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5261512097476009598\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "from tensorflow.io import FixedLenFeature\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "BUFFER_SIZE = 10_000\n",
    "N_THREADS = data.AUTOTUNE\n",
    "\n",
    "\n",
    "def create_tfrecord_dataset(\n",
    "    record_paths: list[str],\n",
    "    batch_size: int = 128,\n",
    "    n_threads: int | None = N_THREADS,\n",
    "    cache: bool = False,\n",
    "    shuffle_buf_size: int | None = None,\n",
    "    seed: int = 42,\n",
    ") -> data.TFRecordDataset:\n",
    "    def __parse_fashion_mnist_tfrecord(record: bytes) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "        parsed = tf.io.parse_example(\n",
    "            record,\n",
    "            features={\n",
    "                \"image\": FixedLenFeature(shape=(), dtype=tf.string, default_value=\"\"),\n",
    "                \"label\": FixedLenFeature(shape=(), dtype=tf.int64),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            tf.ensure_shape(\n",
    "                tf.io.parse_tensor(parsed[\"image\"], out_type=tf.uint8), shape=(28, 28)\n",
    "            ),\n",
    "            tf.cast(parsed[\"label\"], dtype=tf.uint8),\n",
    "        )\n",
    "\n",
    "    dataset = data.TFRecordDataset(record_paths, num_parallel_reads=n_threads).map(\n",
    "        __parse_fashion_mnist_tfrecord, num_parallel_calls=n_threads\n",
    "    )\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buf_size is not None:\n",
    "        dataset = dataset.shuffle(\n",
    "            buffer_size=shuffle_buf_size, seed=SEED, reshuffle_each_iteration=True\n",
    "        )\n",
    "\n",
    "    return dataset.batch(\n",
    "        batch_size, drop_remainder=True, num_parallel_calls=n_threads\n",
    "    ).prefetch(n_threads)\n",
    "\n",
    "\n",
    "train_set = create_tfrecord_dataset(\n",
    "    glob(str(TFR_DIR / \"fashion_mnist\" / \"train\" / \"*.tfrecord\")),\n",
    "    shuffle_buf_size=BUFFER_SIZE,\n",
    ")\n",
    "\n",
    "(norm := layers.Normalization(input_shape=train_set.element_spec[0].shape[1:])).adapt(\n",
    "    train_set.map(lambda X, y: X, num_parallel_calls=N_THREADS)\n",
    ")\n",
    "\n",
    "train_set = train_set.map(lambda X, y: (norm(X), y), num_parallel_calls=N_THREADS)\n",
    "\n",
    "val_set = create_tfrecord_dataset(\n",
    "    glob(str(TFR_DIR / \"fashion_mnist\" / \"validation\" / \"*.tfrecord\")),\n",
    "    cache=True,\n",
    ").map(lambda X, y: (norm(X), y), num_parallel_calls=N_THREADS)\n",
    "\n",
    "test_set = create_tfrecord_dataset(\n",
    "    glob(str(TFR_DIR / \"fashion_mnist\" / \"test\" / \"*.tfrecord\")),\n",
    "    cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "7b7c6ce1-b454-43fd-8a40-42bff4bcbe39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T11:38:32.617091Z",
     "iopub.status.busy": "2024-02-17T11:38:32.617013Z",
     "iopub.status.idle": "2024-02-17T11:38:32.802878Z",
     "shell.execute_reply": "2024-02-17T11:38:32.802513Z",
     "shell.execute_reply.started": "2024-02-17T11:38:32.617084Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "(<tf.Tensor: shape=(128, 28, 28), dtype=float32, numpy=\n",
      "array([[[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]],\n",
      "\n",
      "       [[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]],\n",
      "\n",
      "       [[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]],\n",
      "\n",
      "       [[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]],\n",
      "\n",
      "       [[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 ,  0.7591989 ,  4.9662666 , ...,  2.939328  ,\n",
      "          2.4505823 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466,  5.588145  , ...,  3.2137463 ,\n",
      "          1.4784681 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466,  0.659187  , ...,  0.9461835 ,\n",
      "         -0.3833782 , -0.2408609 ]]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=uint8, numpy=\n",
      "array([3, 1, 9, 8, 6, 9, 3, 4, 8, 0, 1, 5, 1, 6, 0, 5, 8, 9, 5, 6, 2, 3,\n",
      "       7, 2, 2, 4, 7, 3, 6, 2, 3, 0, 5, 8, 0, 5, 1, 9, 1, 2, 1, 1, 6, 8,\n",
      "       8, 3, 5, 3, 4, 1, 5, 9, 6, 0, 1, 5, 1, 7, 5, 2, 8, 0, 6, 4, 7, 7,\n",
      "       8, 8, 8, 0, 3, 2, 5, 2, 9, 7, 0, 8, 7, 3, 5, 6, 5, 6, 7, 2, 8, 7,\n",
      "       1, 1, 8, 0, 9, 9, 9, 5, 5, 0, 6, 6, 7, 3, 8, 3, 4, 9, 5, 7, 6, 5,\n",
      "       5, 9, 3, 7, 4, 6, 4, 4, 0, 9, 5, 4, 4, 4, 5, 3, 9, 8], dtype=uint8)>)\n",
      "\n",
      "Val:\n",
      "(<tf.Tensor: shape=(128, 28, 28), dtype=float32, numpy=\n",
      "array([[[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]],\n",
      "\n",
      "       [[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]],\n",
      "\n",
      "       [[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]],\n",
      "\n",
      "       [[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]],\n",
      "\n",
      "       [[-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        ...,\n",
      "        [-0.1670863 , -0.23698466,  0.06034175, ..., -0.46923783,\n",
      "         -0.30099565, -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ],\n",
      "        [-0.1670863 , -0.23698466, -0.2851459 , ..., -0.46923783,\n",
      "         -0.3833782 , -0.2408609 ]]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=uint8, numpy=\n",
      "array([7, 5, 7, 0, 9, 3, 8, 0, 6, 1, 2, 2, 8, 3, 2, 6, 8, 9, 8, 6, 7, 7,\n",
      "       4, 3, 1, 9, 0, 0, 0, 9, 3, 4, 5, 8, 0, 1, 4, 5, 1, 3, 9, 6, 1, 1,\n",
      "       2, 1, 3, 7, 5, 8, 4, 8, 9, 7, 5, 0, 9, 3, 3, 9, 8, 4, 5, 6, 1, 2,\n",
      "       4, 1, 4, 6, 1, 2, 4, 3, 7, 4, 9, 2, 4, 6, 7, 6, 1, 1, 4, 3, 8, 3,\n",
      "       6, 4, 9, 5, 5, 1, 1, 1, 6, 0, 6, 2, 0, 7, 7, 2, 3, 7, 8, 8, 9, 7,\n",
      "       1, 8, 1, 0, 0, 2, 1, 5, 7, 0, 7, 5, 2, 6, 1, 6, 5, 8], dtype=uint8)>)\n",
      "\n",
      "Test:\n",
      "(<tf.Tensor: shape=(128, 28, 28), dtype=uint8, numpy=\n",
      "array([[[  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   1,   0,   0],\n",
      "        [  0,   0,   1, ...,   2,   0,   0],\n",
      "        ...,\n",
      "        [  0,   0,   5, ..., 221,   1,   0],\n",
      "        [  0,   0,  52, ..., 202,  68,   0],\n",
      "        [  0,   0,  34, ...,  71,  48,   0]],\n",
      "\n",
      "       [[  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0]],\n",
      "\n",
      "       [[  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   1,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0]],\n",
      "\n",
      "       [[  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   2,   0,   0],\n",
      "        ...,\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0]],\n",
      "\n",
      "       [[  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0],\n",
      "        [  0,   0,   0, ...,   0,   0,   0]]], dtype=uint8)>, <tf.Tensor: shape=(128,), dtype=uint8, numpy=\n",
      "array([6, 3, 4, 8, 4, 7, 4, 3, 0, 1, 7, 7, 4, 7, 6, 2, 4, 1, 2, 1, 5, 8,\n",
      "       4, 3, 1, 2, 8, 4, 8, 8, 5, 9, 4, 4, 8, 5, 4, 9, 7, 0, 8, 1, 1, 6,\n",
      "       3, 2, 0, 1, 6, 6, 6, 4, 2, 8, 7, 0, 1, 6, 3, 6, 6, 7, 2, 7, 7, 1,\n",
      "       8, 6, 7, 7, 3, 5, 6, 4, 5, 3, 0, 4, 2, 6, 5, 6, 1, 8, 5, 6, 1, 8,\n",
      "       6, 2, 3, 7, 6, 0, 6, 4, 2, 8, 8, 5, 5, 2, 6, 8, 4, 5, 5, 8, 2, 6,\n",
      "       7, 6, 8, 0, 6, 9, 5, 3, 1, 6, 7, 1, 8, 1, 9, 8, 0, 4], dtype=uint8)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 17:23:32.782728: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-02-17 17:23:32.799287: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for trs in train_set.take(1):\n",
    "    print(\"Train:\")\n",
    "    print(trs)\n",
    "\n",
    "for vs in val_set.take(1):\n",
    "    print(\"\\nVal:\")\n",
    "    print(vs)\n",
    "\n",
    "for tes in test_set.take(1):\n",
    "    print(\"\\nTest:\")\n",
    "    print(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "219ba224-8168-4a4b-ad65-03cb58763ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T11:38:32.803539Z",
     "iopub.status.busy": "2024-02-17T11:38:32.803417Z",
     "iopub.status.idle": "2024-02-17T11:38:32.867988Z",
     "shell.execute_reply": "2024-02-17T11:38:32.867307Z",
     "shell.execute_reply.started": "2024-02-17T11:38:32.803527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_13 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 100)               78500     \n",
      "                                                                 \n",
      " batch_normalization_59 (Ba  (None, 100)               400       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " batch_normalization_60 (Ba  (None, 50)                200       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 50)                2550      \n",
      "                                                                 \n",
      " batch_normalization_61 (Ba  (None, 50)                200       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 10)                510       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87410 (341.45 KB)\n",
      "Trainable params: 87010 (339.88 KB)\n",
      "Non-trainable params: 400 (1.56 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Flatten(input_shape=train_set.element_spec[0].shape[1:]),\n",
    "        layers.Dense(100, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(50, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(50, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=\"nadam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    jit_compile=True,\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9c51dbb7-7f10-4145-ae59-b94c57afe519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T11:38:32.868770Z",
     "iopub.status.busy": "2024-02-17T11:38:32.868647Z",
     "iopub.status.idle": "2024-02-17T11:39:04.976141Z",
     "shell.execute_reply": "2024-02-17T11:39:04.975411Z",
     "shell.execute_reply.started": "2024-02-17T11:38:32.868757Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/17 17:23:32 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n",
      "2024/02/17 17:23:32 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'b6a8ddb40ab24d31a1d6cf80cc25e6c9', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current tensorflow workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "      1/Unknown - 2s 2s/step - loss: 2.8764 - accuracy: 0.1172WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0055s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0055s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 3s 3ms/step - loss: 0.5428 - accuracy: 0.8147 - val_loss: 0.4280 - val_accuracy: 0.8451\n",
      "Epoch 2/1000\n",
      " 42/328 [==>...........................] - ETA: 0s - loss: 0.3706 - accuracy: 0.8664 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 17:23:36.872517: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11953096714435921924\n",
      "2024-02-17 17:23:36.872606: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 37581899012378094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 1s 3ms/step - loss: 0.3637 - accuracy: 0.8688 - val_loss: 0.3688 - val_accuracy: 0.8640\n",
      "Epoch 3/1000\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.3191 - accuracy: 0.8833 - val_loss: 0.3540 - val_accuracy: 0.8711\n",
      "Epoch 4/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.2906 - accuracy: 0.8928 - val_loss: 0.3562 - val_accuracy: 0.8701\n",
      "Epoch 5/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.2716 - accuracy: 0.8989 - val_loss: 0.3422 - val_accuracy: 0.8734\n",
      "Epoch 6/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.2522 - accuracy: 0.9071 - val_loss: 0.3403 - val_accuracy: 0.8802\n",
      "Epoch 7/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.2363 - accuracy: 0.9119 - val_loss: 0.3375 - val_accuracy: 0.8847\n",
      "Epoch 8/1000\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.2229 - accuracy: 0.9164 - val_loss: 0.3389 - val_accuracy: 0.8840\n",
      "Epoch 9/1000\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.2151 - accuracy: 0.9192 - val_loss: 0.3517 - val_accuracy: 0.8808\n",
      "Epoch 10/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.2000 - accuracy: 0.9257 - val_loss: 0.3392 - val_accuracy: 0.8833\n",
      "Epoch 11/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.1899 - accuracy: 0.9290 - val_loss: 0.3507 - val_accuracy: 0.8797\n",
      "Epoch 12/1000\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9320 - val_loss: 0.3398 - val_accuracy: 0.8862\n",
      "Epoch 13/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.1731 - accuracy: 0.9359 - val_loss: 0.3735 - val_accuracy: 0.8816\n",
      "Epoch 14/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.1632 - accuracy: 0.9402 - val_loss: 0.3689 - val_accuracy: 0.8794\n",
      "Epoch 15/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.1556 - accuracy: 0.9420 - val_loss: 0.3744 - val_accuracy: 0.8830\n",
      "Epoch 16/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.1507 - accuracy: 0.9430 - val_loss: 0.3904 - val_accuracy: 0.8786\n",
      "Epoch 17/1000\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.1432 - accuracy: 0.9463 - val_loss: 0.3865 - val_accuracy: 0.8834\n",
      "Epoch 18/1000\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.1347 - accuracy: 0.9492 - val_loss: 0.4071 - val_accuracy: 0.8791\n",
      "Epoch 19/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.1320 - accuracy: 0.9508 - val_loss: 0.3990 - val_accuracy: 0.8803\n",
      "Epoch 20/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.1276 - accuracy: 0.9527 - val_loss: 0.4037 - val_accuracy: 0.8824\n",
      "Epoch 21/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.1196 - accuracy: 0.9554 - val_loss: 0.4241 - val_accuracy: 0.8808\n",
      "Epoch 22/1000\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.1177 - accuracy: 0.9562 - val_loss: 0.4157 - val_accuracy: 0.8840\n",
      "Epoch 23/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.1103 - accuracy: 0.9588 - val_loss: 0.4335 - val_accuracy: 0.8778\n",
      "Epoch 24/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.1069 - accuracy: 0.9603 - val_loss: 0.4329 - val_accuracy: 0.8766\n",
      "Epoch 25/1000\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.1046 - accuracy: 0.9608 - val_loss: 0.4290 - val_accuracy: 0.8831\n",
      "Epoch 26/1000\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 0.0974 - accuracy: 0.9639 - val_loss: 0.4396 - val_accuracy: 0.8810\n",
      "Epoch 27/1000\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.0969 - accuracy: 0.9648 - val_loss: 0.4399 - val_accuracy: 0.8790\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "INFO:tensorflow:Assets written to: /tmp/nix-shell.WhEkng/tmpmj6itgjw/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/nix-shell.WhEkng/tmpmj6itgjw/model/data/model/assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=1000,\n",
    "    validation_data=val_set,\n",
    "    callbacks=[EarlyStopping(patience=20, restore_best_weights=True)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "3614efce-f6b0-45a9-95a3-bfa84823a3cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T11:39:04.977156Z",
     "iopub.status.busy": "2024-02-17T11:39:04.976749Z",
     "iopub.status.idle": "2024-02-17T11:39:05.019027Z",
     "shell.execute_reply": "2024-02-17T11:39:05.018533Z",
     "shell.execute_reply.started": "2024-02-17T11:39:04.977138Z"
    }
   },
   "outputs": [],
   "source": [
    "final_model = keras.Sequential([norm, model])\n",
    "final_model.compile(\n",
    "    optimizer=model.optimizer, loss=model.loss, metrics=model.metrics_names[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "0e924ede-f7ce-4b08-8827-e4766debf8b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T11:39:05.020763Z",
     "iopub.status.busy": "2024-02-17T11:39:05.020555Z",
     "iopub.status.idle": "2024-02-17T11:39:05.293056Z",
     "shell.execute_reply": "2024-02-17T11:39:05.292528Z",
     "shell.execute_reply.started": "2024-02-17T11:39:05.020741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 871us/step - loss: 0.3336 - accuracy: 0.8842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 17:24:05.280093: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14677975496398457193\n",
      "2024-02-17 17:24:05.280123: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6394035354870006688\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = final_model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5adf8-9865-46b2-afea-19a0651b20cc",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41904268-0c85-4bd5-95f7-aadd7011c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# from io import BytesIO\n",
    "\n",
    "# import requests\n",
    "\n",
    "# if not IMDB_DIR.is_dir():\n",
    "#     IMDB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# with BytesIO(\n",
    "#     initial_bytes=requests.get(\n",
    "#         \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "#         allow_redirects=True,\n",
    "#     ).content\n",
    "# ) as archive:\n",
    "#     tar = tarfile.open(fileobj=archive, mode=\"r:gz\")\n",
    "#     tar.extractall(IMDB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352aa0a-9d88-4a8e-be45-ca3b22e0df8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T12:12:18.096881Z",
     "iopub.status.busy": "2024-02-17T12:12:18.096508Z",
     "iopub.status.idle": "2024-02-17T12:12:24.213872Z",
     "shell.execute_reply": "2024-02-17T12:12:24.213445Z",
     "shell.execute_reply.started": "2024-02-17T12:12:18.096846Z"
    }
   },
   "source": [
    "### For Val & Test Sets\n",
    "\n",
    "- 7,500 from both `pos` and `neg` for **Val**\n",
    "- 5,000 from both `pos` and `neg` for **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82ca7d3-7873-46a7-92c0-54816cbc46e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T14:18:14.828882Z",
     "iopub.status.busy": "2024-02-17T14:18:14.828761Z",
     "iopub.status.idle": "2024-02-17T14:18:15.274986Z",
     "shell.execute_reply": "2024-02-17T14:18:15.274731Z",
     "shell.execute_reply.started": "2024-02-17T14:18:14.828873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2274"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "SHUF_BUF_SIZE = 25_000\n",
    "CYCLE_LENGTH = 1000\n",
    "N_THREADS = 8\n",
    "SEED = 42\n",
    "VAL_SIZE = 7500\n",
    "\n",
    "TRAIN_DIR = IMDB_DIR / \"aclImdb\" / \"train\"\n",
    "TEST_DIR = IMDB_DIR / \"aclImdb\" / \"test\"\n",
    "\n",
    "\n",
    "def create_imdb_dataset(\n",
    "    pos_paths: list[str],\n",
    "    neg_paths: list[str],\n",
    "    cycle_len: int = CYCLE_LENGTH,\n",
    "    n_threads: int = N_THREADS,\n",
    "    shuf_buf_size: int | None = None,\n",
    ") -> data.Dataset:\n",
    "    dataset = (\n",
    "        data.Dataset.list_files(pos_paths)\n",
    "        .interleave(\n",
    "            lambda filename: data.TextLineDataset(\n",
    "                filename, num_parallel_reads=n_threads\n",
    "            ).map(\n",
    "                lambda line: (line, tf.constant(1, dtype=tf.uint8)),\n",
    "                num_parallel_calls=n_threads,\n",
    "            ),\n",
    "            cycle_length=CYCLE_LENGTH,\n",
    "            num_parallel_calls=n_threads,\n",
    "        )\n",
    "        .concatenate(\n",
    "            data.Dataset.list_files(neg_paths).interleave(\n",
    "                lambda filename: data.TextLineDataset(\n",
    "                    filename, num_parallel_reads=n_threads\n",
    "                ).map(\n",
    "                    lambda line: (line, tf.constant(0, dtype=tf.uint8)),\n",
    "                    num_parallel_calls=n_threads,\n",
    "                ),\n",
    "                cycle_length=cycle_len,\n",
    "                num_parallel_calls=n_threads,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        dataset.shuffle(buffer_size=shuf_buf_size)\n",
    "        if shuf_buf_size is not None\n",
    "        else dataset\n",
    "    )\n",
    "\n",
    "\n",
    "val_test_pos_paths, val_test_neg_paths = (\n",
    "    np.fromiter(glob(str(TEST_DIR / \"pos\" / \"*.txt\")), dtype=\"object\"),\n",
    "    np.fromiter(glob(str(TEST_DIR / \"neg\" / \"*.txt\")), dtype=\"object\"),\n",
    ")\n",
    "\n",
    "shuf_idx = np.random.default_rng().permutation(len(val_test_pos_paths))\n",
    "val_pos_paths, val_neg_paths = (\n",
    "    val_test_pos_paths[shuf_idx[:VAL_SIZE]],\n",
    "    val_test_neg_paths[shuf_idx[:VAL_SIZE]],\n",
    ")\n",
    "test_pos_paths, test_neg_paths = (\n",
    "    val_test_pos_paths[shuf_idx[VAL_SIZE:]],\n",
    "    val_test_neg_paths[shuf_idx[VAL_SIZE:]],\n",
    ")\n",
    "\n",
    "del val_test_pos_paths, val_test_neg_paths\n",
    "\n",
    "train_set = create_imdb_dataset(\n",
    "    pos_paths=glob(str(TRAIN_DIR / \"pos\" / \"*.txt\")),\n",
    "    neg_paths=glob(str(TRAIN_DIR / \"neg\" / \"*.txt\")),\n",
    "    shuf_buf_size=SHUF_BUF_SIZE,\n",
    ")\n",
    "val_set = create_imdb_dataset(pos_paths=val_pos_paths, neg_paths=val_neg_paths)\n",
    "test_set = create_imdb_dataset(pos_paths=test_pos_paths, neg_paths=test_neg_paths)\n",
    "\n",
    "del val_pos_paths, val_neg_paths, test_pos_paths, test_neg_paths\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b835d-5d56-46fa-bcf0-42497ad7fe71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
